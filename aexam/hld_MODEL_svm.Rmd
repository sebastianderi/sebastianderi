---
title: "Modeling (Support Vector Machine)"
author: Sebastian Deri
output:
  html_document:
    df_print: paged
    code_folding: show
---

[return to [overview page](./hld_OVERVIEW.html)]

In this section, I will build a support vector machine (SVM) model to make
truth and lie predictions on our statements. I turn to this technique because 
it is one of the popular modeling methods in "machine learning". Kuhn & Johnson (2013, p.343) 
state that SVMs have become "one of the most flexible and effective 
machine learning tools available" since they were first developed by Vladimir Vapnik
in the 1960s. Indeed, SVMs are used in countless research papers in the realm of
computational social science and text analysis (e.g. Bakshy, Messing, & Adamic 
(2015) use an SVM for news article classification in their highly cited paper 
on selective news exposure on Facebook, and Wu et al. (2008) list it in their 
article "Top 10 algorithms in data mining", a paper which itself has over 3,900 citations).

I'm not going to pretend to have a deep understanding of the mathematics
that underlie this model (obviously I don't). However, I will try to recapitulate
the main intuitions from those, like Kuhn & Johnson (2013), who have tried to
bring machine learning methods to a larger audience.

In the figure below, Kuhn & Johnson (2013, p.344) have us imagine a case where we are
using two predictor variables (i.e. Predictor A and Predictor B, along the x and y axes)
to predict binary outcomes (i.e. classify/separate the red circles and blue squares).
In cases where these two binary outcomes are perfectly seperable, an infinite number
of lines can be generated that would indeed successfully separate these two classes (left panel).
For each of these individual lines, we can imagine perpendicular lines radiating outwards
from both ends. The distance that each of these lines can radiate outwards before
they would bump into a data point is called the "margin". In each case, the bounds of 
this margin are entirely determined by those nearest abutting data points, which are
called "support vectors" (because they sort of "support" the margin; and hence the name
"support vector machines"). In the simplest case, it is my understanding that support 
vector machines essentially construct a linear model with the largest possible margin
given the data points.

This strategy can then be generalized to cases of higher dimensions (e.g. defining a 2-d 
plane (instead of a 1-d line) to seperate binary outcomes when we have 3 predictors 
(instead of 2) and are thus working in three dimensions, etc). And further, of course 
in most cases, the two classes are not perfectly seperable. However, this problem can be
dealt with by applying a penalties for each of the misclassified data points when
generating maximum margin classifiers. This introduces a new parameter that we must 
set for our model -- the cost penalty for misclassification. (There is no "correct"
cost penalty. And the actual costs penalties used are usually determined through 
another process of training and testing that is "nested" within the training data set,
which cycles through the performance of models with different cost penalties.
A cost penality of 1 penalizes "hits" and "misses" equally. And larger cost penalties
tend to result in more overfitting (Kuhn & Johnson, 2013, p.346-347).)

![](./pics/kuhn_p344_svm.PNG)

Further, we can move from creating models that produce linear classification boundaries
(e.g. lines, planes, and hyperplances, etc) to models that produce non-linear 
classification boundaries through the use of a "kernel function", which takes our 
predictor variables and applies a non-linear transformation to them. Popular non-linear 
kernel functions include the polynomial, radial basis, and hyperbolic tangent 
functions (Kuhn & Johnson, 2013, p.347). Some of these kernel functions introduce
additional parameters that need to be set (e.g. we need to set a free parameter sigma,
when using the radial basis function). 

The different classification boundaries created by a non-linear support vector
machine (which, here, uses a radial basis function) is shown below as a function of
different possible values of the two tunning parameters (the cost penalty and
sigma).

![](kuhn_p347_svm_nonlinear)

Let's now implement a support vector machine to predict truths and lies in our
dataset, using our textual features as predictors. I will implement support
vector machine models with radial basis kernel functions, as I would like to take
advantage of SVMs ability to produce non-linear classification boundaries.

# Packages

Let's make sure to load all the relevant packages.

```{r packages, message=FALSE, warning=FALSE}
# before knitting: message = FALSE, warning = FALSE
library(tidyverse) # cleaning and visualization
library(caret) # modeling
library(kernlab) # has sigest

```

# Load Data

Next, I will again load the pre-processed data, which we created earlier
(see [Data Cleaning & Pre-Processing](./hld_CLEAN.html)). As a reminder,
this dataset has a row for each of 5,004 statements, a column indicating whether 
that particular statement was a truth or a lie, and 90 possible predictor variables for 
each statement, which comes from the textual features we extracted earlier.

```{r load}
# load pre-processed df's
load("stats_proc.Rda")

# For rendering, I'm going to cheat here and load results created when this model was first run
# For some reason, chunks that were supposed to be caches when originally run are rerunning
load("results_svm.Rda")
results <- results_svm # change the specific named (renamed at end), back to generic name

```


# EXAMPLE (Support Vector Machine, with Radial Basis Function)

As usual, let's begin with an example. We will conduct one training-testing
split and create and assess the performance of of one resultant support 
vector machine model, with a radial basis kernel function.

## Split Sample Into Training and Testing Sets

As with our [logistic regression models](./hld_MODEL_logistic.html), we will
conduct a 50-50 training-testing split, randomly allocating one half of the 
statements to the training set, and the other half to the testing set using the
createDataPartition function in the caret package (Kuhn, 2008).

```{r partition}
# set seed, so that statistics don't keep changing for every analysis
set.seed(2019)

# partition data in 50-50 lgocv split (create index for test set)
index_train_ex <- 
  createDataPartition(y = stats_proc$stat_id,
                      p = 0.50,
                      times = 1,
                      list = FALSE)

# actually create data frame with training set (predictors and outcome together)
train_set_ex <- stats_proc[index_train_ex, ]

# actualy create data frame with test set (predictors and outcome together)
test_set_ex <- stats_proc[-index_train_ex, ]

```

## Build Model (on Training Set)

Now that the data are split, we can fit an SVM (radial basis) to the training data.
We can do this by selecting the "svmRadial" model in the  "train" function of
the caret package (Kuhn, 2008; [Kuhn, 2019](https://topepo.github.io/caret/train-models-by-tag.html)).

Unlike with our logistic regression model, we have two free tuning parameters,
which need to be selected. These are our cost penalty and our sigma parameter in
the radial basis function.

The default behavior of the train function for our dataset would be to run through
three different cost penalties: 0.25, 0.50, and 1.0. And the default behavior
of the train function for our dataset would be to select a single sigma value
(rather than trying out several), by taking the mean of the multiple sigma values
suggested for our dataset by the sigest function in the kernlab package. (Oddly,
the default code seems to take the mean of these values, excluding the second suggestion
for no apparent reason [see here for discussion](https://stats.stackexchange.com/questions/408159/what-is-the-basis-for-the-default-sigma-value-used-by-svmradial-in-caret). Thus, I override this and simply take the median of all
the sigma values suggested by the sigest function). Thus, we will hold the 
sigma parameter constant, using the (slightly modified) value derived from the
sigest function. To evaluate a more broad host of cost penalties, I will cycle
through values from: 0.25, 0.5, 1, 2, and 4.

In the end, we must settle on a single value for each of the two free parameters in
our model. Our sigma value is already constant, so this is easy. No choices to make
there. But we must select one cost penalty among our several candidates (0.25, 0.50,
1, 2, and 4). The process of selecting among different parameters is called "tuning".
And it is essentially a nested version of testing/training process, that occurs within
the training dataset we are restricted to here (we will not touch the testing dataset
we set aside earlier). For each possible parameter value (e.g. cost penalty = 0.25),
we can train a model on a subset of the training data and evaluate the performance of
that model on another subset of the training data (essentialy a test set, within
the trainin set). Indeed, for each parameter value, we can train multiple models
on subsets of the training data and pick as our final parameters values, those which
lead to the best performance (here, we will evaluate performance using overall
accuracy; however other metrics could also be used like [AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)).
To create these subsets within our training data we can use the same data splitting
techniques from before (i.e. cross-validation, repeated training/testing splits).
For the sake of consistency, I will use the same data-splitting technique for
model tuning as we used for the overall training-testing split: a 50-50, random
split, and this will be repeated 3 times for each parameter value (performance at
each parameter will be the average across these three rounds).

These model tuning process is completed below, followed by a summary of our results.
As we can see in the textual output, our sigma parameter value was held constant
(at sigma = 0.005704987), and the cost penalty parameter that resulted in the
best performance was a cost penalty of 4. Thus, the svm (radial basis) model we
select will have its free parameters set to: sigma = 0.0057, and cost penalty = 4.

```{r}
# note: these setting chunks are separated for reuse later

# set seed, so that statistics don't keep changing for every analysis
set.seed(2019)

# -----------------------------------------------------------------------------
# STEP 1: SELECT TUNING PARAMETERS

# part a: select cost penalty values
costs_svm <- c(0.25, 0.5, 1, 2, 4)

# part b: get suggested sigma value, for radial basis function, for this data set
# i take the median of the kernlab suggestions 
# (default in caret is to take mean, excluding second suggestion)
# see: https://github.com/topepo/caret/blob/master/models/files/svmRadial.R
# and: https://stats.stackexchange.com/questions/408159/what-is-the-basis-for-the-default-sigma-value-used-by-svmradial-in-caret
sigma_svm <- median(
              kernlab::sigest(
                as.matrix(train_set_ex 
                            %>% select(
                              -stat_id,
                              -grd_truth)
                          ),
                scaled = TRUE
                )
              )

# part c: save parameters in tune grid object
tune_grid_svm <- expand.grid(sigma = sigma_svm,
                             C = costs_svm)

# -----------------------------------------------------------------------------
# STEP 2: SELECT TUNING METHOD
# set up train control object, which specifies training/testing technique
train_control_svm <- trainControl(method = "LGOCV",
                                  number = 3,
                                  p = 0.50)

```

```{r svm_ex, cache = TRUE, eval = FALSE}
# set seed, so that statistics don't keep changing for every analysis
# (applies for models which might have random parameters)
set.seed(2019)

# start timer
start_time <- Sys.time()

# -----------------------------------------------------------------------------
# STEP 3: TRAIN MODEL

# use caret "train" function to train svm
model_ex <- 
  train(form = grd_truth ~ . - stat_id,
        data = train_set_ex,
        method = "svmRadial",
        tuneGrid = tune_grid_svm,
        trControl = train_control_svm,
        metric = "Accuracy") # how to select among models

# end timer
total_time <- Sys.time() - start_time

```

```{r}
# print out overall summary for tuning process
model_ex

```

## Evaluate Model (on Testing Set)

Finally, let's officially evaluate the performance of this SVM model on our
holdout testing set. (Note that while we used a training-testing process within
our training set to select our model parameters, this is not the official metric 
of our model's performance.)

When we evaluate the performance of our tuned model on the holdout testing set 
(consisting of the 2,5004 statements we set aside at the very beginning), we
see that it performs well. 

To do this, we will use it to make
predictions about the remaining 2,504 statments in the test set, which we set aside earlier.
This is done below. The confusionMatrix function from the caret package provides an
easy way to collect some basic statistics on how our model performed. As we can see
from the text output of this function, our model did pretty well (Kuhn, 2008). 
Its overall accuracy was 
significantly better than chance: 60.7% [95% CI: 58.8, 62.6%]. And it performed
well both in identifying truths (i.e. sensitivity: 58.2%) and identifying lies
(i.e. specificity: 63.3%). When it made a prediction that a statement was a truth,
it was correct more often than not (i.e. precision or positive predictive value:
61.3%). And when it made a prediction that a statement was a lie, it was also
correct more often than not (i.e. negative predictive value: 60.2%). (Confidence
intervals can easily be generated for these other four statistics as well (i.e. +/-
z\*(sqrt(p\*(1-p)/n), where z = 1.96 under the normal approximation method
for calculating binomial proportion confidence intervals (Binomial proportion 
confidence interval, 2019); I won't calculate these for this example, but I will 
do so below in our full analysis.)

(Note: some of the point statistics referenced throughout ths analys)


```{r svm_ex_conf, cache = FALSE}
# note: https://stats.stackexchange.com/questions/52274/how-to-choose-a-predictive-model-after-k-fold-cross-validation
# which makes clear that we retrain our final model selected after turning on ALL the training data

# make predictions
preds_ex <-
  predict(object = model_ex,
          newdata = test_set_ex,
          type = "raw")
      
# record model performance
conf_ex <-
  confusionMatrix(data = preds_ex,
                  reference = test_set_ex$grd_truth,
                  positive = "truth")
      
# print confusion matrix
conf_ex

```

# FULL (Predictive Logistic Regression Models)

Our full analysis will almost exactly replicate what we did in our example case
above, except we will replicate the procedure ten times. Thus, we will build
10 different logistic regression models using 10 different training sets and evaluate 
them on their 10 different (corresponding) test sets.

## Run 10 models

Below is the code that runs through this modeling process 10 different times and
saves the result from each round.

```{r svm_full, cache = TRUE, eval = FALSE}
# -----------------------------------------------------------------------------
# STEP 0: set seed, so that statistics don't keep changing for every analysis
set.seed(2019)

# -----------------------------------------------------------------------------
# STEP 1: decide how many times to run the model
rounds <- 10

# -----------------------------------------------------------------------------
# STEP 2: set up object to store results
# part a: create names of results to store
result_cols <- c("model_type", "round", "accuracy", "accuracy_LL", "accuracy_UL",
                 "sensitivity", "specificity", "precision", "npv", "n")

# part b: create matrix
results <-
  matrix(nrow = rounds,
         ncol = length(result_cols))

# part c: actually name columns in results marix
colnames(results) <- result_cols

# part d: convert to df (so multiple variables of different types can be stored)
results <- data.frame(results)

# -----------------------------------------------------------------------------
# STEP 2: start timer
start_time <- Sys.time()

# -----------------------------------------------------------------------------
# STEP 3: create rounds number of models, and store results each time

for (i in 1:rounds){
  
  # part a: partition data in 50-50 lgocv split (create index for test set)
  index_train <- 
    createDataPartition(y = stats_proc$stat_id,
                        p = 0.50,
                        times = 1,
                        list = FALSE)
  
  # part b: create testing and training data sets
  train_set <- stats_proc[index_train, ]
  test_set <- stats_proc[-index_train, ]
  
  
  # part c: use caret "train" function to train logistic regression model
  model <- 
    train(form = grd_truth ~ . - stat_id,
          data = train_set,
          method = "svmRadial")
  
  # part d: make predictions
  preds <-
    predict(object = model,
            newdata = test_set,
            type = "raw")
  
  # part e: store model performance
  conf_m <-
    confusionMatrix(data = preds,
                    reference = test_set$grd_truth,
                    positive = "truth")
  
  # part f: store model results
  # model type
  results[i, 1] <- "svm"
  # round
  results[i, 2] <- i
  # accuracy
  results[i, 3] <- conf_m$overall[1]
  # accuracy LL
  results[i, 4] <- conf_m$overall[3]
  # accuracy UL
  results[i, 5] <- conf_m$overall[4]
  # sensitivity
  results[i, 6] <- conf_m$byClass[1]
  # specificity
  results[i, 7] <- conf_m$byClass[2]
  # precision
  results[i, 8] <- conf_m$byClass[3]
  # negative predictive value
  results[i, 9] <- conf_m$byClass[4]
  # sample size (of test set)
  results[i, 10] <- sum(conf_m$table)
  
  # part g: print round and total elapsed time so far
  cumul_time <- difftime(Sys.time(), start_time, units = "mins")
  print(paste("round #", i, ": cumulative time ", round(cumul_time, 2), " mins",
              sep = ""))
  print("--------------------------------------")

}

```

## View Results (Tabular)

Below, I've displayed a raw tabular summary of the results from each of the 10 models.
As we can see, the results vary somewhat from model to model (e.g. our first model
had an overall accuracy of 60.7%, while our second model had an overall accuracy of
60.2%), although are highly consistent (the variation in our overall performance of
our best peforming model (round 6: 61.5%) and our worst performing model (round 9:
59.4%) is less than 3%).

```{r}
results
  
```

## View Results (Graphically)

Let's visualize average performance across our 10 different models, on some of the
key performance metrics. This is done below. As we can see, over 10 models,
overall accuracy is above chance (with mean performance hovering just below 60%,
and even the lower limit of the confidence interval on this estimate well above 55%).
Similarly, the models performed above chance when predicting make predictions
about statements that were truths and when making predictions about statements
that were lies (confidence intervals for both sensitivity and specificity well
above 50%). And the models were also more reliable than chance when making
a prediction that a statment was a truth and when making a prediction that a
statement was a lie (confidence intervals for precision and npv above 50%).
These results are promising. They reveal that even basic textual features
allow for deciphering of lies from truth.


```{r}
# calculate average sample size
mean_n <- mean(results$n)

# create df to use for visualization
results_viz <-
  results %>%
  group_by(model_type) %>%
  summarize(accuracy = mean(accuracy),
            sensitivity = mean(sensitivity),
            specificity = mean(specificity),
            precision = mean(precision),
            npv = mean(npv)) %>%
  select(-model_type) %>%
  gather(key = "perf_stat",
         value = "value") %>%
  mutate(value = as.numeric(value))

# actual visualization
ggplot(data = results_viz,
  aes(x = perf_stat,
           y = value)) +
geom_point(size = 2,
           color = "#545EDF") +
geom_errorbar(aes(ymin = (value - 1.96*sqrt(value*(1-value)/mean_n)),
                   ymax = (value + 1.96*sqrt(value*(1-value)/mean_n))),
              color = "#545EDF",
              width = 0.15,
              size = 1.25) +
geom_hline(yintercept = 0.5,
           linetype = "dashed",
           size = 0.5,
           color = "red") +
scale_y_continuous(breaks = seq(from = 0, to = 1, by = 0.05),
                   limits = c(0, 1)) +
scale_x_discrete(limits = rev(c("accuracy", "sensitivity", "specificity", 
                            "precision", "npv"))) + 
coord_flip() +
theme(panel.grid.major.x = element_line(color = "grey",
                                        size = 0.25),
      panel.grid.minor.x = element_blank(),
      panel.grid.major.y = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank(),
      plot.title = element_text(hjust = 0.5),
      axis.title.y = element_text(margin = 
                                    margin(t = 0, r = 10, b = 0, l = 0)),
      axis.title.x = element_text(margin = 
                                    margin(t = 10, r = 00, b = 0, l = 0)),
      axis.text.x = element_text(angle = 90)) +
labs(title = "Performance Statistics (Support Vector Machine)",
     x = "Performance Statistic",
     y = "Proportion (0 to 1)")


```

# Save Results

```{r, eval = FALSE}
# rename results df, to be particular to this model type (for disambiguation later)
results_svm <- results

# clear results variable
 rm(results)

# save results in Rda file
save(model_ex,
     results_svm,
     file = "results_svm.Rda")
```

# Render

Again, some chunks which take long to evaluate are not evaluating and instead saved/loaded from
current directory. Rendering with: rmarkdown::render("hld_MODEL_svm.Rmd")

# Citations

* Bakshy, E., Messing, S., & Adamic, L. A. (2015). Exposure to ideologically 
diverse news and opinion on Facebook. Science, 348(6239), 1130-1132. 
https://doi.org/10.1126/science.aaa1160

* Kuhn, M., & Johnson, K. (2013). Applied predictive modeling (Vol. 26). Springer.

* Wu, X., Kumar, V., Quinlan, J. R., Ghosh, J., Yang, Q., Motoda, H., . Philip, S. Y. (2008). 
Top 10 algorithms in data mining. Knowledge and Information Systems, 14(1), 1-37.


# END
