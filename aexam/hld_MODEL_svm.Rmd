---
title: "Modeling (Support Vector Machine)"
author: Sebastian Deri
output:
  html_document:
    df_print: paged
    code_folding: show
---

[return to [overview page](./hld_OVERVIEW.html)]

In this section, we will build a support vector machine (SVM) model to make
truth and lie predictions on our statements. We turn to this technique because 
it is one of the popular modeling methods in "machine learning". Kuhn & Johnson (2013, p.343) 
state that SVMs have become "one of the most flexible and effective 
machine learning tools available" since they were first developed by Vladimir Vapnik
in the 1960s. Indeed, SVMs are used in countless research papers in the realm of
computational social science and text analysis (e.g. Bakshy, Messing, & Adamic 
(2015) use an SVM for news article classification in their highly cited paper 
on selective news exposure on Facebook, and Wu et al. (2008) list it in their 
article "Top 10 algorithms in data mining", a paper which itself has over 3,900 citations).

I'm not going to pretend to have a deep understanding of the mathematics
that underlie this model (obviously I don't). However, I will try to recapitulate
the main intuitions from those, like Kuhn & Johnson (2013), who have tried to
bring machine learning methods to a larger audience.

In the figure below, Kuhn & Johnson (2013, p.344) have us imagine a case where we are
using two predictor variables (i.e. Predictor A and Predictor B, along the x and y axes)
to predict binary outcomes (i.e. classify/separate the red circles and blue squares).
In cases where these two binary outcomes are perfectly seperable, an infinite number
of lines can be generated that would indeed successfully separate these two classes (left panel).
For each of these individual lines, we can imagine perpendicular lines radiating outwards
from both ends. The distance that each of these lines can radiate outwards before
they would bump into a data point is called the "margin". In each case, the bounds of 
this margin are entirely determined by those nearest abutting data points, which are
called "support vector" (because they sort of "support" the margin). In the simplest
case, it is my understanding that support vector machines attempt to find the 

This strategy can then be generalized to cases of higher dimensions (e.g. defining a plane in
when we have 3 predictors and are thus working in three dimensions, etc). And further, in
most cases, of course, the classes are not perfectly seperable and thus in finding our
maximum margin classifier we can apply penalties as the number of misclassified data points increase.
This introduces a new parameter that we must set for our model -- the cost, or penalty
for misclassification. (There is no "correct" cost penalty, and these are usually determined
through another process of training and testing "nested" within the training data set.
A cost penality of 1 penalizes "hits" and "misses" equally. Larger cost penalties
tend to result in more overfitting (Kuhn & Johnson, 2013, p.346-347).)

![](./pics/kuhn_p344_svm.PNG)

Further, we can move from creating linear classification boundaries (e.g. lines,
planes, and hyperplances, etc) to non-linear classification boundaries through the
use of a "kernel function", which takes our predictor variables and applies a
non-linear transformation to them. Popular non-linear kernel functions include
the polynomial, radial basis, and hyperbolic tangent functions (Kuhn & Johnson, 
2013, p.347). Some of these kernel functions introduce additional parameters that
we need to set for our model (e.g. we need to set a free parameter sigma, if using
the radial basis function). 

The different classification boundaries created by a non-linear support vector
machine (which uses a radial basis function) is shown below as a function of
different possible values of the tunning parameters.

![](kuhn_p347_svm_nonlinear)

Let's now implement a support vector machine to predict truths and lies in our
dataset, using our textual features as predictors. I will use the a support
vector machine with a radial basis function, as I would like to take advantage of
SVMs ability to produce non-linear classification boundaries.

# Packages

Let's make sure to load all the relevant packages.

```{r packages, message=FALSE, warning=FALSE}
# before knitting: message = FALSE, warning = FALSE
library(tidyverse) # cleaning and visualization
library(caret) # modeling
library(kernlab) # has sigest

```

# Load Data

Next, I will again load the pre-processed data, which we created earlier
(see [Data Cleaning & Pre-Processing](./hld_CLEAN.html)). As a reminder,
this dataset has a row for each of 5,004 statements, a column indicating whether 
that particular statement was a truth or a lie, and 90 possible predictor variables for 
each statement, which comes from the textual features we extracted earlier.

```{r load}
# load pre-processed df's
load("stats_proc.Rda")

# For rendering, I'm going to cheat here and load results created when this model was first run
# For some reason, chunks that were supposed to be caches when originally run are rerunning
load("results_svm.Rda")
results <- results_svm # change the specific named (renamed at end), back to generic name

```


# EXAMPLE (Single Support Vector Machine Model)

As usual, let's begin with an example. Here we will simply train and test
one single logistic regression model.

## Split Sample Into Training and Testing Sets

Our first step will be to split the entire dataset into two parts -- our training
data set, on which the model will be build, and our testing data set, on which
the performance of our model will be evaluated. Although many possible splits would be
acceptable (e.g. 75-25, 90-10), we are going to conduct an exact 50-50 split,
randomly allocating one half of the statements to the training set, and
the other half to the testing set. The createDataPartition function in the 
caret packages makes this easy (Kuhn, 2008).

```{r partition}
# set seed, so that statistics don't keep changing for every analysis
set.seed(2019)

# partition data in 50-50 lgocv split (create index for test set)
index_train_ex <- 
  createDataPartition(y = stats_proc$stat_id,
                      p = 0.50,
                      times = 1,
                      list = FALSE)

# actually create data frame with training set (predictors and outcome together)
train_set_ex <- stats_proc[index_train_ex, ]

# actualy create data frame with test set (predictors and outcome together)
test_set_ex <- stats_proc[-index_train_ex, ]

```

## Build Model (on Training Set)

Now that the data are split, we can fit a logistic regression model to the training
data. Again, the caret package makes this easy with its "train" function (Kuhn, 2008),
which allows us to select from over 238 different model type (Kuhn, 2019; see:
[Chapter 7](https://topepo.github.io/caret/train-models-by-tag.html), including
of course the logistic regression model from the family of general lineal models.
A single logistic regression model is fitted below.


```{r svm_ex, cache = TRUE, eval = FALSE}
# set seed, so that statistics don't keep changing for every analysis
# (applies for models which might have random parameters)
set.seed(2019)

# start timer
start_time <- Sys.time()

# tuning parameters
tune_params <- 

# set up training 
train_control_svm <- trainControl(method = "LGOCV",
                                  number = 3)

# get suggest sigma value
# i take the median of the kernlab suggestions 
# (default in caret is to take mean, exc)
sigma_svm <- median(
              kernlab::sigest(
                as.matrix(train_set_ex 
                            %>% select(
                              -stat_id,
                              -grd_truth)
                          ),
                scaled = TRUE
                )
              )



# use caret "train" function to train logistic regression model
model_ex <- 
  train(form = grd_truth ~ . - stat_id,
        data = train_set_ex,
        method = "svmRadial"),
        tuneGrid = expand.grid(# sigma = sigma_svm,
                               C = c(1)),
        trControl = trainControl(method = "LGOCV",
                                  number = 3))

# end timer
total_time <- Sys.time() - start_time

```


```{r}
attributes(model_ex)

```

```{r}
model_ex$finalModel
```


## Evaluate Model (on Testing Set)

Finally, let's see if our model is any good. To do this, we will use it to make
predictions about the remaining 2,504 statments in the test set, which we set aside earlier.
This is done below. The confusionMatrix function from the caret package provides an
easy way to collect some basic statistics on how our model performed. As we can see
from the text output of this function, our model did pretty well (Kuhn, 2008). 
Its overall accuracy was 
significantly better than chance: 60.7% [95% CI: 58.8, 62.6%]. And it performed
well both in identifying truths (i.e. sensitivity: 58.2%) and identifying lies
(i.e. specificity: 63.3%). When it made a prediction that a statement was a truth,
it was correct more often than not (i.e. precision or positive predictive value:
61.3%). And when it made a prediction that a statement was a lie, it was also
correct more often than not (i.e. negative predictive value: 60.2%). (Confidence
intervals can easily be generated for these other four statistics as well (i.e. +/-
z\*(sqrt(p\*(1-p)/n), where z = 1.96 under the normal approximation method
for calculating binomial proportion confidence intervals (Binomial proportion 
confidence interval, 2019); I won't calculate these for this example, but I will 
do so below in our full analysis.)

(Note: some of the point statistics referenced throughout ths analys)


```{r svm_ex_conf, cache = FALSE}
# make predictions
preds_ex <-
  predict(object = model_ex,
          newdata = test_set_ex,
          type = "raw")
      
# record model performance
conf_ex <-
  confusionMatrix(data = preds_ex,
                  reference = test_set_ex$grd_truth,
                  positive = "truth")
      
# print confusion matrix
conf_ex

```

# FULL (Predictive Logistic Regression Models)

Our full analysis will almost exactly replicate what we did in our example case
above, except we will replicate the procedure ten times. Thus, we will build
10 different logistic regression models using 10 different training sets and evaluate 
them on their 10 different (corresponding) test sets.

## Run 10 models

Below is the code that runs through this modeling process 10 different times and
saves the result from each round.

```{r svm_full, cache = TRUE, eval = FALSE}
# # -----------------------------------------------------------------------------
# STEP 0: set seed, so that statistics don't keep changing for every analysis
set.seed(2019)

# # -----------------------------------------------------------------------------
# STEP 1: decide how many times to run the model
rounds <- 10

# -----------------------------------------------------------------------------
# STEP 2: set up object to store results
# part a: create names of results to store
result_cols <- c("model_type", "round", "accuracy", "accuracy_LL", "accuracy_UL",
                 "sensitivity", "specificity", "precision", "npv", "n")

# part b: create matrix
results <-
  matrix(nrow = rounds,
         ncol = length(result_cols))

# part c: actually name columns in results marix
colnames(results) <- result_cols

# part d: convert to df (so multiple variables of different types can be stored)
results <- data.frame(results)

# -----------------------------------------------------------------------------
# STEP 2: start timer
start_time <- Sys.time()

# -----------------------------------------------------------------------------
# STEP 3: create rounds number of models, and store results each time

for (i in 1:rounds){
  
  # part a: partition data in 50-50 lgocv split (create index for test set)
  index_train <- 
    createDataPartition(y = stats_proc$stat_id,
                        p = 0.50,
                        times = 1,
                        list = FALSE)
  
  # part b: create testing and training data sets
  train_set <- stats_proc[index_train, ]
  test_set <- stats_proc[-index_train, ]
  
  
  # part c: use caret "train" function to train logistic regression model
  model <- 
    train(form = grd_truth ~ . - stat_id,
          data = train_set,
          method = "svmRadial")
  
  # part d: make predictions
  preds <-
    predict(object = model,
            newdata = test_set,
            type = "raw")
  
  # part e: store model performance
  conf_m <-
    confusionMatrix(data = preds,
                    reference = test_set$grd_truth,
                    positive = "truth")
  
  # part f: store model results
  # model type
  results[i, 1] <- "svm"
  # round
  results[i, 2] <- i
  # accuracy
  results[i, 3] <- conf_m$overall[1]
  # accuracy LL
  results[i, 4] <- conf_m$overall[3]
  # accuracy UL
  results[i, 5] <- conf_m$overall[4]
  # sensitivity
  results[i, 6] <- conf_m$byClass[1]
  # specificity
  results[i, 7] <- conf_m$byClass[2]
  # precision
  results[i, 8] <- conf_m$byClass[3]
  # negative predictive value
  results[i, 9] <- conf_m$byClass[4]
  # sample size (of test set)
  results[i, 10] <- sum(conf_m$table)
  
  # part g: print round and total elapsed time so far
  cumul_time <- difftime(Sys.time(), start_time, units = "mins")
  print(paste("round #", i, ": cumulative time ", round(cumul_time, 2), " mins",
              sep = ""))
  print("--------------------------------------")

}

```

## View Results (Tabular)

Below, I've displayed a raw tabular summary of the results from each of the 10 models.
As we can see, the results vary somewhat from model to model (e.g. our first model
had an overall accuracy of 60.7%, while our second model had an overall accuracy of
60.2%), although are highly consistent (the variation in our overall performance of
our best peforming model (round 6: 61.5%) and our worst performing model (round 9:
59.4%) is less than 3%).

```{r}
results
  
```

## View Results (Graphically)

Let's visualize average performance across our 10 different models, on some of the
key performance metrics. This is done below. As we can see, over 10 models,
overall accuracy is above chance (with mean performance hovering just below 60%,
and even the lower limit of the confidence interval on this estimate well above 55%).
Similarly, the models performed above chance when predicting make predictions
about statements that were truths and when making predictions about statements
that were lies (confidence intervals for both sensitivity and specificity well
above 50%). And the models were also more reliable than chance when making
a prediction that a statment was a truth and when making a prediction that a
statement was a lie (confidence intervals for precision and npv above 50%).
These results are promising. They reveal that even basic textual features
allow for deciphering of lies from truth.


```{r}
# calculate average sample size
mean_n <- mean(results$n)

# create df to use for visualization
results_viz <-
  results %>%
  group_by(model_type) %>%
  summarize(accuracy = mean(accuracy),
            sensitivity = mean(sensitivity),
            specificity = mean(specificity),
            precision = mean(precision),
            npv = mean(npv)) %>%
  select(-model_type) %>%
  gather(key = "perf_stat",
         value = "value") %>%
  mutate(value = as.numeric(value))

# actual visualization
ggplot(data = results_viz,
  aes(x = perf_stat,
           y = value)) +
geom_point(size = 2,
           color = "#545EDF") +
geom_errorbar(aes(ymin = (value - 1.96*sqrt(value*(1-value)/mean_n)),
                   ymax = (value + 1.96*sqrt(value*(1-value)/mean_n))),
              color = "#545EDF",
              width = 0.15,
              size = 1.25) +
geom_hline(yintercept = 0.5,
           linetype = "dashed",
           size = 0.5,
           color = "red") +
scale_y_continuous(breaks = seq(from = 0, to = 1, by = 0.05),
                   limits = c(0, 1)) +
scale_x_discrete(limits = rev(c("accuracy", "sensitivity", "specificity", 
                            "precision", "npv"))) + 
coord_flip() +
theme(panel.grid.major.x = element_line(color = "grey",
                                        size = 0.25),
      panel.grid.minor.x = element_blank(),
      panel.grid.major.y = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank(),
      plot.title = element_text(hjust = 0.5),
      axis.title.y = element_text(margin = 
                                    margin(t = 0, r = 10, b = 0, l = 0)),
      axis.title.x = element_text(margin = 
                                    margin(t = 10, r = 00, b = 0, l = 0)),
      axis.text.x = element_text(angle = 90)) +
labs(title = "Performance Statistics (Support Vector Machine)",
     x = "Performance Statistic",
     y = "Proportion (0 to 1)")


```

# Save Results

```{r, eval = FALSE}
# rename results df, to be particular to this model type (for disambiguation later)
results_svm <- results

# clear results variable
 rm(results)

# save results in Rda file
save(model_ex,
     results_svm,
     file = "results_svm.Rda")
```

# Render

Again, some chunks which take long to evaluate are not evaluating and instead saved/loaded from
current directory. Rendering with: rmarkdown::render("hld_MODEL_svm.Rmd")

# Citations

* Bakshy, E., Messing, S., & Adamic, L. A. (2015). Exposure to ideologically 
diverse news and opinion on Facebook. Science, 348(6239), 1130-1132. 
https://doi.org/10.1126/science.aaa1160

* Kuhn, M., & Johnson, K. (2013). Applied predictive modeling (Vol. 26). Springer.

* Wu, X., Kumar, V., Quinlan, J. R., Ghosh, J., Yang, Q., Motoda, H., . Philip, S. Y. (2008). 
Top 10 algorithms in data mining. Knowledge and Information Systems, 14(1), 1-37.


# END
