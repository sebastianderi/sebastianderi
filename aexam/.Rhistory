predict(object = model_non_hybrid,
newdata = test_set_ex,
type = "raw")
# examine hybrid model performance
conf_hybrid <-
confusionMatrix(data = preds_hybrid,
reference = test_set_ex$grd_truth,
positive = "truth")
# print
conf_hybrid
# examine non-hybrid model performance
conf_non_hybrid <-
confusionMatrix(data = preds_non_hybrid,
reference = test_set_ex$grd_truth,
positive = "truth")
# print
conf_non_hybrid
# calculate values relevant for test (and some extras, in case i want to compare sensitivity, etc)
hybrid_truth_correct = conf_hybrid$table[4]
hybrid_truth_total = conf_hybrid$table[3] + conf_hybrid$table[4]
hybrid_lie_correct = conf_hybrid$table[1]
hybrid_lie_total = conf_hybrid$table[1] + conf_hybrid$table[2]
hybrid_total_correct = hybrid_truth_correct + hybrid_lie_correct
hyrbid_total_truth_guesses = conf_hybrid$table[2] + conf_hybrid$table[4]
hyrbid_total_lie_guesses = conf_hybrid$table[1] + conf_hybrid$table[3]
hybrid_total = sum(conf_hybrid$table)
non_hybrid_truth_correct = conf_non_hybrid$table[4]
non_hybrid_truth_total = conf_non_hybrid$table[3] + conf_non_hybrid$table[4]
non_hybrid_lie_correct = conf_non_hybrid$table[1]
non_hybrid_lie_total = conf_non_hybrid$table[1] + conf_non_hybrid$table[2]
non_hybrid_total_correct = non_hybrid_truth_correct + non_hybrid_lie_correct
non_Hyrbid_total_truth_guesses = conf_non_hybrid$table[2] + conf_non_hybrid$table[4]
non_hyrbid_total_lie_guesses = conf_non_hybrid$table[1] + conf_non_hybrid$table[3]
non_hybrid_total = sum(conf_non_hybrid$table)
# conduct actual test
prop.test(x = c(hybrid_total_correct, non_hybrid_total_correct),
n = c(hybrid_total, non_hybrid_total),
alternative = "two.sided",
conf.level = 0.95)
# examine z value, using two proportion z-test just for comparison
# function to compute z-value
# from: https://www.r-bloggers.com/comparison-of-two-proportions-parametric-z-test-and-non-parametric-chi-squared-methods/
z.prop = function(x1,x2,n1,n2){
numerator = (x1/n1) - (x2/n2)
p.common = (x1+x2) / (n1+n2)
denominator = sqrt(p.common * (1-p.common) * (1/n1 + 1/n2))
z.prop.ris = numerator / denominator
return(z.prop.ris)
}
# actual z prop (z = 0.58)
z_value <-
z.prop(x1 = hybrid_total_correct,
x2 = non_hybrid_total_correct,
n1 = hybrid_total,
n2 = non_hybrid_total)
# get corresponding p-value (two-sided) (p = 0.559)
p_value <- 2*pnorm(-abs(z_value))
# print z and p
print(paste("z=", round(z_value, 3),
", p=", round(p_value, 3),
sep = ""))
# combine: grd_truth, non_hybrid predictions, hybrd predictions
preds_comp <-
cbind(as.data.frame(test_set_ex$grd_truth),
as.data.frame(preds_non_hybrid),
as.data.frame(preds_hybrid))
# label columns
colnames(preds_comp) <- c("grd_truth", "non_hyb", "hyb")
# print
# preds_comp
# generate additional information
preds_tally <-
preds_comp %>%
group_by(grd_truth, non_hyb, hyb) %>%
summarize(n = n()) %>%
arrange(desc(grd_truth), desc(non_hyb), desc(hyb)) %>%
mutate(change_present = case_when(non_hyb == hyb ~ "no",
non_hyb != hyb ~ "yes")) %>%
mutate(change_type = case_when(grd_truth == "truth" & non_hyb == "truth" & hyb == "truth" ~ "TP -> TP",
grd_truth == "truth" & non_hyb == "truth" & hyb == "lie" ~ "TP -> FN",
grd_truth == "truth" & non_hyb == "lie" & hyb == "truth" ~ "FN -> TP",
grd_truth == "truth" & non_hyb == "lie" & hyb == "lie" ~ "FN -> FN",
grd_truth == "lie" & non_hyb == "truth" & hyb == "truth" ~ "FP -> FP",
grd_truth == "lie" & non_hyb == "truth" & hyb == "lie" ~ "FP -> TN",
grd_truth == "lie" & non_hyb == "lie" & hyb == "truth" ~ "TN -> FP",
grd_truth == "lie" & non_hyb == "lie" & hyb == "lie" ~ "TN -> TN")) %>%
mutate(change_effect = case_when(change_type == "TP -> TP" ~ "nothing",
change_type == "TN -> TN" ~ "nothing",
change_type == "FP -> FP" ~ "nothing",
change_type == "FN -> FN" ~ "nothing",
change_type == "FN -> TP" ~ "gain",
change_type == "FP -> TN" ~ "gain",
change_type == "TP -> FN" ~ "loss",
change_type == "TN -> FP" ~ "loss")) %>%
arrange(change_effect)
# print
preds_tally
# tally the total number of times each type of "change_effect" occurred
preds_tally %>%
group_by(change_effect) %>%
summarize(n = sum(n))
# # -----------------------------------------------------------------------------
# STEP 0: set seed, so that statistics don't keep changing for every analysis
set.seed(2019)
# # -----------------------------------------------------------------------------
# STEP 1: decide how many times to run the model
rounds <- 10
# -----------------------------------------------------------------------------
# STEP 2: set up object to store results
# part a: create names of results to store
result_cols <- c("model_type", "hyb_type", "round", "accuracy", "accuracy_LL", "accuracy_UL",
"sensitivity", "specificity", "precision", "npv", "n",
"total_truths", "total_lies", "total_truth_guesses", "total_lie_guesses")
# part b: create matrix
results <-
matrix(nrow = rounds,
ncol = length(result_cols))
# part c: actually name columns in results marix
colnames(results) <- result_cols
# part d: convert to df (so multiple variables of different types can be stored)
results <- data.frame(results)
# -----------------------------------------------------------------------------
# STEP 2: start timer
start_time <- Sys.time()
# -----------------------------------------------------------------------------
# STEP 3: create rounds number of models, and store results each time
for (i in 1:rounds){
# part a: partition data in 50-50 lgocv split (create index for test set)
index_train <-
createDataPartition(y = stats_combo$stat_id,
p = 0.50,
times = 1,
list = FALSE)
# part b: create testing and training data sets
train_set <- stats_combo[index_train, ]
test_set <- stats_combo[-index_train, ]
# part c: use caret "train" function to train hybrid and non-hybrid logistic regression model
model_hybrid_i <-
train(form = grd_truth ~ . - stat_id,
data = train_set,
method = "glm",
family = "binomial")
model_non_hybrid_i <-
train(form = grd_truth ~ . -stat_id -predict,
data = train_set,
method = "glm",
family = "binomial")
# part d: make predictions, using both hybrid and non-hybrid models
preds_hyb_i <-
predict(object = model_hybrid_i,
newdata = test_set,
type = "raw")
preds_non_hyb_i <-
predict(object = model_non_hybrid_i,
newdata = test_set,
type = "raw")
# part e: store model performance of both hybrid and non-hybrid model
conf_hyb_i <-
confusionMatrix(data = preds_hyb_i,
reference = test_set$grd_truth,
positive = "truth")
conf_non_hyb_i <-
confusionMatrix(data = preds_non_hyb_i,
reference = test_set$grd_truth,
positive = "truth")
# part f: store hybrid and non-hybrod model results
# model type
results[2*i-1, 1] <- "logistic"
results[2*i, 1] <- "logistic"
# hybrid o non-hybrid
results[2*i-1, 2] <- "hybrid"
results[2*i, 2] <- "non_hybrid"
# round
results[2*i-1, 3] <- i
results[2*i, 3] <- i
# accuracy
results[2*i-1, 4] <- conf_hyb_i$overall[1]
results[2*i, 4] <- conf_non_hyb_i$overall[1]
# accuracy LL
results[2*i-1, 5] <- conf_hyb_i$overall[3]
results[2*i, 5] <- conf_non_hyb_i$overall[3]
# accuracy UL
results[2*i-1, 6] <- conf_hyb_i$overall[4]
results[2*i, 6] <- conf_non_hyb_i$overall[4]
# sensitivity
results[2*i-1, 7] <- conf_hyb_i$byClass[1]
results[2*i, 7] <- conf_non_hyb_i$byClass[1]
# specificity
results[2*i-1, 8] <- conf_hyb_i$byClass[2]
results[2*i, 8] <- conf_non_hyb_i$byClass[2]
# precision
results[2*i-1, 9] <- conf_hyb_i$byClass[3]
results[2*i, 9] <- conf_non_hyb_i$byClass[3]
# negative predictive value
results[2*i-1, 10] <- conf_hyb_i$byClass[4]
results[2*i, 10] <- conf_non_hyb_i$byClass[4]
# sample size (total)
results[2*i-1, 11] <- sum(conf_hyb_i$table)
results[2*i, 11] <- sum(conf_non_hyb_i$table)
# total truths
results[2*i-1, 12] <- conf_hyb_i$table[3] + conf_hyb_i$table[4]
results[2*i, 12] <- conf_non_hyb_i$table[3] + conf_hyb_i$table[4]
# total lies
results[2*i-1, 13] <- conf_hyb_i$table[1] + conf_hyb_i$table[2]
results[2*i, 13] <- conf_non_hyb_i$table[1] + conf_non_hyb_i$table[2]
# total truth guesses
results[2*i-1, 14] <- conf_hyb_i$table[2] + conf_hyb_i$table[4]
results[2*i, 14] <- conf_non_hyb_i$table[2] + conf_non_hyb_i$table[4]
# total lie guesses
results[2*i-1, 15] <- conf_hyb_i$table[1] + conf_hyb_i$table[3]
results[2*i, 15] <- conf_non_hyb_i$table[1] + conf_non_hyb_i$table[3]
# part g: print round and total elapsed time so far
cumul_time <- difftime(Sys.time(), start_time, units = "mins")
print(paste("round #", i, ": cumulative time ", round(cumul_time, 2), " mins",
sep = ""))
print("--------------------------------------")
}
# print results (long form)
results
# get data in wide format (with accuracy for hybrid and non_hybrid models next to each other)
results_acc_wide <-
results %>%
select(hyb_type, round, accuracy) %>%
spread(key = hyb_type,
value = accuracy)
# print
results_acc_wide %>%
mutate(winner = case_when(hybrid > non_hybrid ~ "hybrid",
hybrid < non_hybrid ~ "non-hybrid",
hybrid == non_hybrid ~ "tie"))
# sign test, via binomial test
binom.test(x = 10,
n = 10,
p = 0.5)
# conduct Wilcoxon signed-rank test
wilcox.test(x = results_acc_wide$hybrid,
y = results_acc_wide$non_hybrid,
paired = TRUE,
conf.int = 0.95)
# average n in test set for hybrid and non-hybrid models
total_hyb <- mean((results %>% filter(hyb_type == "hybrid"))$n)
total_non_hyb <- mean((results %>% filter(hyb_type == "non_hybrid"))$n)
# vector of denominators to use, when creating error bars
denoms_overall <- c(total_hyb,
total_non_hyb)
# visualize overall accuracy
results %>%
group_by(hyb_type) %>%
summarize(accuracy = mean(accuracy)) %>%
ggplot(aes(x = hyb_type,
y = accuracy)) +
geom_point(size = 2,
color = "#545EDF") +
geom_errorbar(aes(ymin = accuracy - 1.96*sqrt(accuracy*(1-accuracy)/denoms_overall),
ymax = accuracy + 1.96*sqrt(accuracy*(1-accuracy)/denoms_overall)),
color = "#545EDF",
width = 0.05,
size = 1) +
geom_hline(yintercept = 0.5,
linetype = "dashed",
size = 0.5,
color = "red") +
scale_y_continuous(breaks = seq(from = 0.49, to = 0.70, by = 0.01),
limits = c(0.49, 0.70)) +
scale_x_discrete(limits = c("non_hybrid", "hybrid")) +
theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.major.y = element_line(color = "grey",
size = 0.25),
panel.background = element_blank(),
axis.ticks = element_blank(),
plot.title = element_text(hjust = 0.5),
axis.title.y = element_text(margin =
margin(t = 0, r = 10, b = 0, l = 0)),
axis.title.x = element_text(margin =
margin(t = 10, r = 00, b = 0, l = 0))) +
labs(title = "Accuracy by Model Type",
x = "Model Type",
y = "Overall Accuracy")
# average n in test set for hybrid and non-hybrid models
total_truths_hyb <- mean((results %>% filter(hyb_type == "hybrid"))$total_truths)
total_truths_non_hyb <- mean((results %>% filter(hyb_type == "non_hybrid"))$total_truths)
total_lies_hyb <- mean((results %>% filter(hyb_type == "hybrid"))$total_lies)
total_lies_non_hyb <- mean((results %>% filter(hyb_type == "non_hybrid"))$total_lies)
total_truth_guesses_hyb <- mean((results %>% filter(hyb_type == "hybrid"))$total_truth_guesses)
total_truth_guesses_non_hyb <- mean((results %>% filter(hyb_type == "non_hybrid"))$total_truth_guesses)
total_lie_guesses_hyb <- mean((results %>% filter(hyb_type == "hybrid"))$total_lie_guesses)
total_lie_guesses_non_hyb <- mean((results %>% filter(hyb_type == "non_hybrid"))$total_lie_guesses)
# vector of denominators to use, when creating error bars
denoms_perf <- c(total_truths_non_hyb,
total_truths_hyb,
total_lies_non_hyb,
total_lies_hyb,
total_truth_guesses_non_hyb,
total_truth_guesses_hyb,
total_lie_guesses_non_hyb,
total_lie_guesses_hyb)
# visualize results
results %>%
select(hyb_type, round, sensitivity, specificity, precision, npv) %>%
gather(key = "metric",
value = "value",
sensitivity, specificity, precision, npv) %>%
group_by(hyb_type, metric) %>%
summarize(value = mean(value)) %>%
ungroup() %>%
mutate(metric = factor(metric,
levels = c("sensitivity", "specificity", "precision", "npv"))) %>%
ggplot(aes(x = hyb_type,
y = value)) +
geom_point(size = 2,
color = "#545EDF") +
geom_errorbar(aes(ymin = value - 1.96*sqrt(value*(1-value)/denoms_perf),
ymax = value + 1.96*sqrt(value*(1-value)/denoms_perf)),
color = "#545EDF",
width = 0.05,
size = 1) +
geom_hline(yintercept = 0.5,
linetype = "dashed",
size = 0.5,
color = "red") +
scale_y_continuous(breaks = seq(from = 0.50, to = 0.70, by = 0.05),
limits = c(0.49, 0.70)) +
scale_x_discrete(limits = c("non_hybrid", "hybrid")) +
facet_grid(metric ~ .) +
theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.major.y = element_line(color = "grey",
size = 0.25),
plot.background = element_blank(),
panel.background = element_blank(),
panel.border = element_rect(colour = "black", fill=NA, size=1),
axis.ticks = element_blank(),
plot.title = element_text(hjust = 0.5),
axis.title.y = element_text(margin =
margin(t = 0, r = 10, b = 0, l = 0)),
axis.title.x = element_text(margin =
margin(t = 10, r = 00, b = 0, l = 0))) +
labs(title = "Metrics by Model Type",
x = "Model Type",
y = "Proportion")
# rename results df, to be particular to this model type (for disambiguation later)
results_HYB_log <- results
# clear results variable
rm(results)
# save results in Rda file
save(results_HYB_log,
file = "results_HYB_log.Rda")
# load df of combined human and processed textual feature and ground truth
load("stats_combo.Rda")
# print
stats_combo
# note: these setting chunks are separated for reuse later
# set seed, so that statistics don't keep changing for every analysis
set.seed(2019)
# -----------------------------------------------------------------------------
# STEP 1: SELECT TUNING PARAMETERS
# part a: set range of tuning parameters (layer size and weight decay)
tune_grid_neural <- expand.grid(size = c(1:5, 10),
decay = c(0, 0.05, 0.1, 1, 2))
# part b: set some other consrains to be imposed on network (to keep computation manageable)
# see: p. 361 of Kuhn & Johnson (2013,
max_size_neaural <- max(tune_grid_neural$size)
max_weights_neural <- max_size_neaural*(nrow(train_set_ex) + 1) + max_size_neaural + 1
# before knitting: message = FALSE, warning = FALSE
library(tidyverse) # cleaning and visualization
library(caret) # modeling
library(nnet) # for neural networks specifically (caret "nnet" wraps these functions)
# load df of combined human and processed textual feature and ground truth
load("stats_combo.Rda")
# For rendering, I'm going to cheat here and load results created when this model was first run
# For some reason, chunks that were supposed to be cached when originally run are rerunning
load("results_HYB_neural.Rda")
# load df of combined human and processed textual feature and ground truth
load("stats_combo.Rda")
# print
stats_combo
# set seed, so that statistics don't keep changing for every analysis
set.seed(2019)
# partition data in 50-50 lgocv split (create index for test set)
index_train_ex <-
createDataPartition(y = stats_combo$stat_id,
p = 0.50,
times = 1,
list = FALSE)
# actually create data frame with training set (predictors and outcome together)
train_set_ex <- stats_combo[index_train_ex, ]
# actualy create data frame with test set (predictors and outcome together)
test_set_ex <- stats_combo[-index_train_ex, ]
# note: these setting chunks are separated for reuse later
# set seed, so that statistics don't keep changing for every analysis
set.seed(2019)
# -----------------------------------------------------------------------------
# STEP 1: SELECT TUNING PARAMETERS
# part a: set range of tuning parameters (layer size and weight decay)
tune_grid_neural <- expand.grid(size = c(1:5, 10),
decay = c(0, 0.05, 0.1, 1, 2))
# part b: set some other consrains to be imposed on network (to keep computation manageable)
# see: p. 361 of Kuhn & Johnson (2013,
max_size_neaural <- max(tune_grid_neural$size)
max_weights_neural <- max_size_neaural*(nrow(train_set_ex) + 1) + max_size_neaural + 1
# -----------------------------------------------------------------------------
# STEP 2: SELECT TUNING METHOD
# set up train control object, which specifies training/testing technique
train_control_neural <- trainControl(method = "LGOCV",
number = 3,
p = 0.50)
# set seed, so that statistics don't keep changing for every analysis
# (applies for models which might have random parameters)
set.seed(2019)
# start timer
start_time <- Sys.time()
# -----------------------------------------------------------------------------
# STEP 3: TRAIN MODEL
# use caret "train" function to train svm
model_hybrid <-
train(form = grd_truth ~ . - stat_id,
data = train_set_ex,
method = "nnet",
tuneGrid = tune_grid_neural,
trControl = train_control_neural,
metric = "Accuracy", # how to select among models
trace = FALSE,
maxit = 100,
MaxNWts = max_weights_neural) # don't print output along the way
model_non_hybrid <-
train(form = grd_truth ~ . -stat_id -predict,
data = train_set_ex,
method = "nnet",
tuneGrid = tune_grid_neural,
trControl = train_control_neural,
metric = "Accuracy", # how to select among models
trace = FALSE,
maxit = 100,
MaxNWts = max_weights_neural) # don't print output along the way
# end timer
total_time <- Sys.time() - start_time
total_time
# examine tuning parameters selected for non-hybrid model
model_non_hybrid
# examine tuning parameters select for hybrid model
model_hybrid
# generate predictions for hybrid model
preds_hybrid <-
predict(object = model_hybrid,
newdata = test_set_ex,
type = "raw")
# generate predictions for non-hybrid model
preds_non_hybrid <-
predict(object = model_non_hybrid,
newdata = test_set_ex,
type = "raw")
# examine hybrid model performance
conf_hybrid <-
confusionMatrix(data = preds_hybrid,
reference = test_set_ex$grd_truth,
positive = "truth")
# print
conf_hybrid
# examine non-hybrid model performance
conf_non_hybrid <-
confusionMatrix(data = preds_non_hybrid,
reference = test_set_ex$grd_truth,
positive = "truth")
# print
conf_non_hybrid
dim(test_set_ex)
# calculate values relevant for test (and some extras, in case i want to compare sensitivity, etc)
hybrid_truth_correct = conf_hybrid$table[4]
hybrid_truth_total = conf_hybrid$table[3] + conf_hybrid$table[4]
hybrid_lie_correct = conf_hybrid$table[1]
hybrid_lie_total = conf_hybrid$table[1] + conf_hybrid$table[2]
hybrid_total_correct = hybrid_truth_correct + hybrid_lie_correct
hyrbid_total_truth_guesses = conf_hybrid$table[2] + conf_hybrid$table[4]
hyrbid_total_lie_guesses = conf_hybrid$table[1] + conf_hybrid$table[3]
hybrid_total = sum(conf_hybrid$table)
non_hybrid_truth_correct = conf_non_hybrid$table[4]
non_hybrid_truth_total = conf_non_hybrid$table[3] + conf_non_hybrid$table[4]
non_hybrid_lie_correct = conf_non_hybrid$table[1]
non_hybrid_lie_total = conf_non_hybrid$table[1] + conf_non_hybrid$table[2]
non_hybrid_total_correct = non_hybrid_truth_correct + non_hybrid_lie_correct
non_Hyrbid_total_truth_guesses = conf_non_hybrid$table[2] + conf_non_hybrid$table[4]
non_hyrbid_total_lie_guesses = conf_non_hybrid$table[1] + conf_non_hybrid$table[3]
non_hybrid_total = sum(conf_non_hybrid$table)
# conduct actual test
prop.test(x = c(hybrid_total_correct, non_hybrid_total_correct),
n = c(hybrid_total, non_hybrid_total),
alternative = "two.sided",
conf.level = 0.95)
# examine z value, using two proportion z-test just for comparison
# function to compute z-value
# from: https://www.r-bloggers.com/comparison-of-two-proportions-parametric-z-test-and-non-parametric-chi-squared-methods/
z.prop = function(x1,x2,n1,n2){
# compute z-value
numerator = (x1/n1) - (x2/n2)
p.common = (x1+x2) / (n1+n2)
denominator = sqrt(p.common * (1-p.common) * (1/n1 + 1/n2))
z.prop.ris = numerator / denominator
# compute p-value
p_value <- 2*pnorm(-abs(z.prop.ris))
# combine results in string vector
test_result <- c(paste("z=", round(z.prop.ris, 3),
", p=", round(p_value, 3),
sep = ""))
# return z and p-value
return(test_result)
}
# get actual two proportion z-test results
z.prop(x1 = hybrid_total_correct,
x2 = non_hybrid_total_correct,
n1 = hybrid_total,
n2 = non_hybrid_total)
