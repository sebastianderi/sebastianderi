print(paste("round #", i, ": cumulative time ", round(cumul_time, 2), " mins",
sep = ""))
print("--------------------------------------")
}
results
# calculate average sample size
mean_n <- mean(results$n)
# create df to use for visualization
results_viz <-
results %>%
group_by(model_type) %>%
summarize(accuracy = mean(accuracy),
sensitivity = mean(sensitivity),
specificity = mean(specificity),
precision = mean(precision),
npv = mean(npv)) %>%
select(-model_type) %>%
gather(key = "perf_stat",
value = "value") %>%
mutate(value = as.numeric(value))
# actual visualization
ggplot(data = results_viz,
aes(x = perf_stat,
y = value)) +
geom_point(size = 2,
color = "#545EDF") +
geom_errorbar(aes(ymin = (value - 1.96*sqrt(value*(1-value)/mean_n)),
ymax = (value + 1.96*sqrt(value*(1-value)/mean_n))),
color = "#545EDF",
width = 0.15,
size = 1.25) +
geom_hline(yintercept = 0.5,
linetype = "dashed",
size = 0.5,
color = "red") +
scale_y_continuous(breaks = seq(from = 0, to = 1, by = 0.05),
limits = c(0, 1)) +
scale_x_discrete(limits = rev(c("accuracy", "sensitivity", "specificity",
"precision", "npv"))) +
coord_flip() +
theme(panel.grid.major.x = element_line(color = "grey",
size = 0.25),
panel.grid.minor.x = element_blank(),
panel.grid.major.y = element_blank(),
panel.background = element_blank(),
axis.ticks = element_blank(),
plot.title = element_text(hjust = 0.5),
axis.title.y = element_text(margin =
margin(t = 0, r = 10, b = 0, l = 0)),
axis.title.x = element_text(margin =
margin(t = 10, r = 00, b = 0, l = 0)),
axis.text.x = element_text(angle = 90)) +
labs(title = "Performance Statistics (Neural Net)",
x = "Performance Statistic",
y = "Proportion (0 to 1)")
# rename results df, to be particular to this model type (for disambiguation later)
results_neural <- results
# again, we're going to kind of cheat here for easier rendering later
# (by saving the model_ex object from the sample; but let's rename it to avoid confusion)
model_ex_neural <- model_ex
save(model_ex_neural,
results_neural,
file = "results_neural.Rda")
# before knitting: message = FALSE, warning = FALSE
library(tidyverse) # cleaning and visualization
library(caret) # modeling
# load pre-processed df's
load("stats_proc.Rda")
# For rendering, I'm going to cheat here and load results created when this model was first run
# For some reason, chunks that were supposed to be caches when originally run are rerunning
load("results_svm.Rda")
results <- results_svm # change the specific named (renamed at end), back to generic name
# set seed, so that statistics don't keep changing for every analysis
set.seed(2019)
# partition data in 50-50 lgocv split (create index for test set)
index_train_ex <-
createDataPartition(y = stats_proc$stat_id,
p = 0.50,
times = 1,
list = FALSE)
# actually create data frame with training set (predictors and outcome together)
train_set_ex <- stats_proc[index_train_ex, ]
# actualy create data frame with test set (predictors and outcome together)
test_set_ex <- stats_proc[-index_train_ex, ]
model_ex
attributes(model_ex)
model_ex$finalModel
data.frame(.C = c(.25, .5, 1),
.sigma = .05)
sigest(train_set_ex)
library(kernlab) # has sigest
sigest(train_set_ex)
sigest(as.matrix(train_set_ex))
sigest(as.matrix(train_set_ex))
train_set_ex
sigest(as.matrix(train_set_ex %>% select(-stat_id, -grd_truth)))
median(sigest(as.matrix(train_set_ex %>% select(-stat_id, -grd_truth))))
?train
train
model_ex <-
train(form = grd_truth ~ . - stat_id,
data = train_set_ex,
method = "svmRadial",
tuneGrid = expand.grid(.sigma = median(sigest(train_set_ex)),
.C = c(0.25, 0.50, 1, 2, 4)),
trControl = trainControl(method = "LGOCV",
number = 3))
expand.grid(.sigma = median(sigest(train_set_ex)),
.C = c(0.25, 0.50, 1, 2, 4))
# get suggest sigma value
median(sigest(as.matrix(train_set_ex)))
sigest(as.matrix(train_set_ex))
# get suggest sigma value
median(sigest(as.matrix(train_set_ex %>% select(-stat_id, -grd_truth))))
sigma_svm <- median(
sigest(
as.matrix(train_set_ex
%>% select(
-stat_id,
-grd_truth)
)
)
)
model_ex <-
train(form = grd_truth ~ . - stat_id,
data = train_set_ex,
method = "svmRadial",
tuneGrid = expand.grid(.sigma = sigma_svm,
.C = c(0.25, 0.50, 1, 2, 4)),
trControl = trainControl(method = "LGOCV",
number = 3))
model_ex$finalModel
preds_ex <-
predict(object = model_ex,
newdata = test_set_ex,
type = "raw")
# record model performance
conf_ex <-
confusionMatrix(data = preds_ex,
reference = test_set_ex$grd_truth,
positive = "truth")
# print confusion matrix
conf_ex
median(sigest(as.matrix(train_set_ex %>% select(-stat_id, -grd_truth)), scaled = TRUE))
# set seed, so that statistics don't keep changing for every analysis
# (applies for models which might have random parameters)
set.seed(2019)
# start timer
start_time <- Sys.time()
# tuning parameters
tune_params <-
# set up training
train_control_svm <- trainControl(method = "LGOCV",
number = 3)
# get suggest sigma value
sigma_svm <- median(
kernlab::sigest(
as.matrix(train_set_ex
%>% select(
-stat_id,
-grd_truth)
),
scaled = TRUE
)
)
# use caret "train" function to train logistic regression model
model_ex <-
train(form = grd_truth ~ . - stat_id,
data = train_set_ex,
method = "svmRadial",
tuneGrid = expand.grid(.sigma = sigma_svm,
.C = c(1)),
trControl = trainControl(method = "LGOCV",
number = 3))
# end timer
total_time <- Sys.time() - start_time
total_time
model_ex$finalModel
# set seed, so that statistics don't keep changing for every analysis
# (applies for models which might have random parameters)
set.seed(2019)
# start timer
start_time <- Sys.time()
# tuning parameters
tune_params <-
# set up training
train_control_svm <- trainControl(method = "LGOCV",
number = 3)
# get suggest sigma value
sigma_svm <- median(
kernlab::sigest(
as.matrix(train_set_ex
%>% select(
-stat_id,
-grd_truth)
),
scaled = TRUE
)
)
# use caret "train" function to train logistic regression model
model_ex <-
train(form = grd_truth ~ . - stat_id,
data = train_set_ex,
method = "svmRadial",
tuneGrid = expand.grid(sigma = sigma_svm,
C = c(1)),
trControl = trainControl(method = "LGOCV",
number = 3))
# end timer
total_time <- Sys.time() - start_time
model_ex
# make predictions
preds_ex <-
predict(object = model_ex,
newdata = test_set_ex,
type = "raw")
# record model performance
conf_ex <-
confusionMatrix(data = preds_ex,
reference = test_set_ex$grd_truth,
positive = "truth")
# print confusion matrix
conf_ex
model_ex <-
train(form = grd_truth ~ . - stat_id,
data = train_set_ex,
method = "svmRadial",
tuneGrid = expand.grid(# sigma = sigma_svm,
C = c(1)),
trControl = trainControl(method = "LGOCV",
number = 3))
sigma_svm
sigma_svm[-2]
kernlab::sigest(
as.matrix(train_set_ex
%>% select(
-stat_id,
-grd_truth)
),
scaled = TRUE
)
kernlab::sigest(
as.matrix(train_set_ex
%>% select(
-stat_id,
-grd_truth)
),
scaled = TRUE
)[-2]
c(1, 2, 3, 4, 5)[-2]
c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)[-2]
model_ex$modelInfo
model_ex$method
model_ex$modelType
model_ex$results
model_ex$pred
model_ex$bestTune
model_ex$call
model_ex$dots
model_ex$metric
model_ex$control
model_ex$finalModel
model_ex$preProcess
model_ex$trainingData
model_ex$resample
model_ex$resampledCM
model_ex$perfNames
model_ex$maximize
model_ex$yLimits
model_ex$times
model_ex$levels
model_ex$terms
model_ex$coefnames
model_ex$xlevels
model_ex <-
train(form = grd_truth ~ . - stat_id,
data = train_set_ex,
method = "svmRadial")
model_ex
0.006038915
mean(
kernlab::sigest(
as.matrix(train_set_ex
%>% select(
-stat_id,
-grd_truth)
),
scaled = TRUE
)[-2]
)
# make predictions
preds_ex <-
predict(object = model_ex,
newdata = test_set_ex,
type = "raw")
# record model performance
conf_ex <-
confusionMatrix(data = preds_ex,
reference = test_set_ex$grd_truth,
positive = "truth")
# print confusion matrix
conf_ex
mean(c(0, 0, 0, 1))
mean(c(0, 50, 0, 0)[-2])
# before knitting: message = FALSE, warning = FALSE
library(tidyverse) # cleaning and visualization
library(caret) # modeling
library(kernlab) # has sigest
load("stats_proc.Rda")
# set seed, so that statistics don't keep changing for every analysis
set.seed(2019)
# partition data in 50-50 lgocv split (create index for test set)
index_train_ex <-
createDataPartition(y = stats_proc$stat_id,
p = 0.50,
times = 1,
list = FALSE)
# actually create data frame with training set (predictors and outcome together)
train_set_ex <- stats_proc[index_train_ex, ]
# actualy create data frame with test set (predictors and outcome together)
test_set_ex <- stats_proc[-index_train_ex, ]
c(44, 45, 46, 47)[-2]
c(44, 45, 46, 47)[-1]
c(44, 45, 46, 47)[-1:2]
c(44, 45, 46, 47)[-2:-1]
2 ^((1:NULL) - 3)
2 ^((1:1) - 3)
2 ^((1:3) - 3)
?trainControl
# set seed, so that statistics don't keep changing for every analysis
# (applies for models which might have random parameters)
set.seed(2019)
# start timer
start_time <- Sys.time()
# tuning parameters
tune_params <-
# set up training
train_control_svm <- trainControl(method = "LGOCV",
number = 3,
p = 0.50)
# get suggest sigma value
# i take the median of the kernlab suggestions
# (default in caret is to take mean, exc)
sigma_svm <- median(
kernlab::sigest(
as.matrix(train_set_ex
%>% select(
-stat_id,
-grd_truth)
),
scaled = TRUE
)
)
# use caret "train" function to train logistic regression model
model_ex <-
train(form = grd_truth ~ . - stat_id,
data = train_set_ex,
method = "svmRadial"),
# set seed, so that statistics don't keep changing for every analysis
# (applies for models which might have random parameters)
set.seed(2019)
# start timer
start_time <- Sys.time()
# -----------------------------------------------------------------------------
# STEP 1: SELECT TUNING PARAMETERS
# part a: select cost penalty values
costs_svm <- c(0.25, 0.5, 1, 2, 4)
# part b: get suggested sigma value, for radial basis function, for this data set
# i take the median of the kernlab suggestions
# (default in caret is to take mean, excluding second suggestion)
# see: https://github.com/topepo/caret/blob/master/models/files/svmRadial.R
# and: https://stats.stackexchange.com/questions/408159/what-is-the-basis-for-the-default-sigma-value-used-by-svmradial-in-caret
sigma_svm <- median(
kernlab::sigest(
as.matrix(train_set_ex
%>% select(
-stat_id,
-grd_truth)
),
scaled = TRUE
)
)
# load pre-processed df's
load("stats_proc.Rda")
# before knitting: message = FALSE, warning = FALSE
library(tidyverse) # cleaning and visualization
library(caret) # modeling
library(kernlab) # has sigest
# set seed, so that statistics don't keep changing for every analysis
set.seed(2019)
# partition data in 50-50 lgocv split (create index for test set)
index_train_ex <-
createDataPartition(y = stats_proc$stat_id,
p = 0.50,
times = 1,
list = FALSE)
# actually create data frame with training set (predictors and outcome together)
train_set_ex <- stats_proc[index_train_ex, ]
# actualy create data frame with test set (predictors and outcome together)
test_set_ex <- stats_proc[-index_train_ex, ]
# set seed, so that statistics don't keep changing for every analysis
# (applies for models which might have random parameters)
set.seed(2019)
# start timer
start_time <- Sys.time()
# -----------------------------------------------------------------------------
# STEP 1: SELECT TUNING PARAMETERS
# part a: select cost penalty values
costs_svm <- c(0.25, 0.5, 1, 2, 4)
# part b: get suggested sigma value, for radial basis function, for this data set
# i take the median of the kernlab suggestions
# (default in caret is to take mean, excluding second suggestion)
# see: https://github.com/topepo/caret/blob/master/models/files/svmRadial.R
# and: https://stats.stackexchange.com/questions/408159/what-is-the-basis-for-the-default-sigma-value-used-by-svmradial-in-caret
sigma_svm <- median(
kernlab::sigest(
as.matrix(train_set_ex
%>% select(
-stat_id,
-grd_truth)
),
scaled = TRUE
)
)
# part c: save parameters in tune grid object
tune_grid_svm <- expand.grid(sigma = sigma_svm,
C = costs_svm)
# -----------------------------------------------------------------------------
# STEP 2: SELECT TUNING METHOD
# set up train control object, which specifies training/testing technique
train_control_svm <- trainControl(method = "LGOCV",
number = 3,
p = 0.50)
# -----------------------------------------------------------------------------
# STEP 3: TRAIN MODEL
# use caret "train" function to train svm
model_ex <-
train(form = grd_truth ~ . - stat_id,
data = train_set_ex,
method = "svmRadial",
tuneGrid = tune_grid_svm,
trControl = train_control_svm,
metric = "Accuracy") # how to select among models
# end timer
total_time <- Sys.time() - start_time
model_ex
attributes(model_ex)
# set seed, so that statistics don't keep changing for every analysis
# (applies for models which might have random parameters)
set.seed(2019)
# start timer
start_time <- Sys.time()
# -----------------------------------------------------------------------------
# STEP 1: SELECT TUNING PARAMETERS
# part a: select cost penalty values
costs_svm <- c(0.25, 0.5, 1, 2, 4)
# part b: get suggested sigma value, for radial basis function, for this data set
# i take the median of the kernlab suggestions
# (default in caret is to take mean, excluding second suggestion)
# see: https://github.com/topepo/caret/blob/master/models/files/svmRadial.R
# and: https://stats.stackexchange.com/questions/408159/what-is-the-basis-for-the-default-sigma-value-used-by-svmradial-in-caret
sigma_svm <- median(
kernlab::sigest(
as.matrix(train_set_ex
%>% select(
-stat_id,
-grd_truth)
),
scaled = TRUE
)
)
# part c: save parameters in tune grid object
tune_grid_svm <- expand.grid(sigma = sigma_svm,
C = costs_svm)
# -----------------------------------------------------------------------------
# STEP 2: SELECT TUNING METHOD
# set up train control object, which specifies training/testing technique
train_control_svm <- trainControl(method = "LGOCV",
number = 3,
p = 0.50)
# -----------------------------------------------------------------------------
# STEP 3: TRAIN MODEL
# use caret "train" function to train svm
model_ex <-
train(form = grd_truth ~ . - stat_id,
data = train_set_ex,
method = "svmRadial",
tuneGrid = tune_grid_svm,
trControl = train_control_svm,
metric = "Accuracy") # how to select among models
# end timer
total_time <- Sys.time() - start_time
model_ex
# make predictions
preds_ex <-
predict(object = model_ex,
newdata = test_set_ex,
type = "raw")
# record model performance
conf_ex <-
confusionMatrix(data = preds_ex,
reference = test_set_ex$grd_truth,
positive = "truth")
# print confusion matrix
conf_ex
attributes(model_ex)
model_ex$finalModel
model_ex$trainingData
model_ex$resample
model_ex
mean(model_ex$resample$Accuracy)
model_ex$finalModel
